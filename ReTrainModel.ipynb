{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow med Omtr√§ning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifiera Sv√•ra Bilder f√∂r Omtr√§ning\n",
    "L√§gg till felklassificerade bilder i en lista och kopiera dem till en ny mapp f√∂r omtr√§ning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import psutil\n",
    "\n",
    "# Kolla batteristatus\n",
    "battery = psutil.sensors_battery()\n",
    "if battery and battery.power_plugged is False:\n",
    "    print(\"‚ö†Ô∏è Warning: Your laptop is running on battery. Performance may be reduced, \"\n",
    "          \"and CUDA might fail due to power-saving features.\")\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available: Running on {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is not available: Running on CPU. Ensure your GPU drivers are installed correctly.\")\n",
    "\n",
    "# Define paths\n",
    "train_fire_dir = \"dataset/train/fire\"\n",
    "train_non_fire_dir = \"dataset/train/non_fire\"\n",
    "retrain_fire_dir = \"dataset/retrain/fire\"\n",
    "retrain_non_fire_dir = \"dataset/retrain/non_fire\"\n",
    "\n",
    "# ‚úÖ Define transformation (same as before)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ladda in data fr√•n olika kataloger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Custom Dataset to Load from Multiple Directories\n",
    "class FireDataset(Dataset):\n",
    "    def __init__(self, fire_dirs, non_fire_dirs, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load images from all fire directories\n",
    "        for fire_dir in fire_dirs:\n",
    "            for img_file in os.listdir(fire_dir):\n",
    "                img_path = os.path.join(fire_dir, img_file)\n",
    "                self.data.append((img_path, 1))  # Fire = 1\n",
    "\n",
    "        # Load images from all non-fire directories\n",
    "        for non_fire_dir in non_fire_dirs:\n",
    "            for img_file in os.listdir(non_fire_dir):\n",
    "                img_path = os.path.join(non_fire_dir, img_file)\n",
    "                self.data.append((img_path, 0))  # Non-Fire = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ‚úÖ Load dataset dynamically from both directories\n",
    "fire_dirs = [train_fire_dir, retrain_fire_dir]\n",
    "non_fire_dirs = [train_non_fire_dir, retrain_non_fire_dir]\n",
    "\n",
    "train_dataset = FireDataset(fire_dirs, non_fire_dirs, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"üîÑ Training dataset now includes {len(train_dataset)} images (incl. retraining data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ladda In Data F√∂r Omtr√§ning\n",
    "Vi laddar in originaldatasetet plus de extra sv√•ra bilderna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# ‚úÖ Load MobileNetV2 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = models.MobileNet_V2_Weights.DEFAULT\n",
    "model = models.mobilenet_v2(weights=weights)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
    "model.to(device)\n",
    "\n",
    "# ‚úÖ Define loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ‚úÖ Retrain the model with expanded dataset\n",
    "num_epochs_retrain = 10\n",
    "for epoch in range(num_epochs_retrain):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_retrain}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "# ‚úÖ Save the retrained model\n",
    "torch.save(model.state_dict(), \"fire_classifier_retrained.pth\")\n",
    "print(\"‚úÖ Retraining complete! Model saved as 'fire_classifier_retrained.pth'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load test dataset\n",
    "test_dirs = {\n",
    "    \"fire\": \"dataset/test/fire\",\n",
    "    \"non_fire\": \"dataset/test/non_fire\"\n",
    "}\n",
    "\n",
    "true_labels, pred_labels = [], []\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "for label, folder in test_dirs.items():\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            _, predicted_class = torch.max(probabilities, 1)\n",
    "\n",
    "        true_labels.append(1 if label == \"fire\" else 0)\n",
    "        pred_labels.append(predicted_class.item())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "class_names = [\"non_fire\", \"fire\"]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix: Retrained Model\")\n",
    "plt.savefig(\"confusion_matrix_retrained.png\")\n",
    "print(\"üìä New confusion matrix saved as 'confusion_matrix_retrained.png'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
